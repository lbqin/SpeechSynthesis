# 深度学习于语音合成研究综述

本文综述近年来深度学习用于语音合成的一些方法。

### WaveNet

在自回归生成模型在图像和文本领域广泛应用的时候，WaveNet [4] 尝试将这些思想应用于语音领域。仿照[PixelRNN](https://arxiv.org/pdf/1601.06759) (van den Oord et al., 2016)图像生成的做法， WaveNet依据之前采样点来生成下一个采样点。生成下一个采样点的模型为CNN结构。为了生成指定说话人的声音，以及生成指定文本的声音，引入了全局条件和局部条件，来控制合成内容。为了扩大感受野，带洞卷积，使filter的按照指数扩张。

WaveNet存在的问题是，1） 每次预测一个采样点，速度太慢；2）如果用于TTS，那初始采样点选择将会很重要；3）以及需要文本前端的支持，前端分析出错，将直接影响合成效果。

### Deep Voice 1

WaveNet之后，百度第一代Deep Voice出现了。为了解决速度慢这个问题，我们看看百度在Deep Voice第一代 [1] 是怎么做的。

百度deep voice的做法是仿照传统参数合成的各个步骤，将每一阶段用一个神经网络模型来代替。那整个模型就是一个大的神经网络。拆开成多个子模块独立进行好处是：1) 每一阶段为一个独立模型，单独训练更见容易；2) 调试方便，如果合成出错，单独调试某个模块就可以；3) 可以方便人为控制，如果某个模块输出错误，还可以通过人工的方式纠正；4) 每一模块都是一个神经网络，这样不需要太多人工特征，让模型自己学，减少专家知识。

和目前（截至百度发表此论文）已有论文，主要是WaveNet, SampleRNN [5], Char2Wav [6]相比，百度优势如下：

1. 完整性。百度提供了一套完整的TTS解决方案，用的人工特征少，而WaveNet, SampleRNN, Char2Wav这些方法需要依赖于，一个现有TTS的部分功能模块，为其提供特征。
2. 实时性好。

虽然百度用的特征少，但训练数据中也需要一些标注，包括：重音、基频（用Praat工具产生）。

Deep voice将语音合成分成5各部分进行，分别是：手写体转音素（G2P）、音频切分、音素时长预测、基频率预测、声学模型，下面我们按照传统参数合成分工方式，看看其每一模块原理：

#### 前端

参数合成前端往往是分析文本全上下文标签的，比如WaveNet使用的局部条件[4]，使用了[HTS full format labels](https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/F0parametrisation/hts_lab_format.pdf)(Zen, 2006)；如果去看看就知道有多复杂了，对每一个音素，一共需要知道53组特征，包括词性、是否发音、是否重读、元音、音节数量等等。

百度Deep voice的前端相对简单很多，只需要知道音素、重音标注、音素发音时长、基频F0。简单的好处是，容易快速适配到新数据集。为了得到上述四组特征，使用了下列几个模型：

1) **G2P**目标是得到音素。手写体转音素使用的是encoder-decoder结构，基于的是[Yao & Zweig](https://arxiv.org/pdf/1506.00196) (2015)那篇文章。 

2) **音频切分**目标是得到音素和音频的对齐信息，知道每句话中每个音素在对应音频中的起点和终点。使用的方法是语音识别[Deep Speech 2](https://arxiv.org/abs/1512.02595)中的对齐方法([Amodei et al., 2015](https://openreview.net/pdf?id=XL9vPjMAjuXB8D1RUG6L))。这些对齐信息只是用来训练时长模型，inference阶段不需要使用。为了让对齐更加精细，使用类似bigram的组合方式。

3) **时长预测和基频预测**模型为同一个模型多任务预测方式。模型输入为带有重音标注的音素序列，输出为时长、是否发音概率、基频F0。

得到这些特征之后，全部输入到声学模型中合成语音。

#### 声学模型

WaveNet合成很慢，百度改进了一下，达到400倍加速。前端得到的特征作为WaveNet的局部条件，以合成指定文本。百度对WaveNet改进方向主要是：改变网络层数、残差通道数、用矩阵乘代替上采样卷积、CPU优化、GPU优化等。

#### 评价

1. 误差传递和累计。五大子模块组成的TTS系统，一个模块出错，整个合成就有误，同时误差会累计。
2. 系统复杂。要开发和调试好五个模块并非简单。
3. 虽然已经大大减少了人工特征数量，但仍要使用音素标记、重音标记、F0等特征。
4. 声学模型本质仍是一个WaveNet，虽然百度进行了一些性能优化，但没有根本性解决速度慢的问题。可以认为提速400倍主要是进行了大量优化（去重复性计算、缓存计算结果、多线程、异步平行等），而非网络结构上的突破。
5. 受限于G2P模型，不能解决多音单词问题。虽然多音词单词在英文中少见，但事实上也是存在的（eg., read）。用词典来训练G2P可以解决OOV (out-of-vocabulary) 问题，但不能解决多音字问题，这在中文里问题更显著。这一问题没有解决，距离产品化有些路程。

### Tacotron

无论语音合成前端或者参数合成各个阶段，都需要大量领域知识，有许多设计技巧。Tacotron [7]探索了一种端到端的方式，输入文本，直接输出语音。使用端到端语音合成好处如下：

1. 减少特征工程，只需要输入文本即可，其他特征模型自己学习
2. 各种条件方便添加，例如语种、说话人、情感等
3. 避免了多个子模型的误差传递和积累

一方面，Tacotron是一个生成式模型，截至Tacotron发布，目前已有的使用生成式模型于TTS的论文有：WaveNet和DeepVoice，前者需要文本分析前端，后者整体上看是端到端，但每一部分单独训练，并非端到端训练。

另一方面，Tacotron是一个端到端模型，截至Tacotron发布，目前已有的端到端模型有：Wang [8]、Char2Wav [6]。但是，Wang需要一个预训练的HMM模型用于对齐，模型预测的是声学参数，仍需要vocoder合成语音；Char2Wav可以直接输入字符进行合成，但是仍是预测出声学参数，需要使用vococder（文中使用了SampleRNN [5] 作为vocoder）进行合成语音。

可以认为Tacotron是最端到端语音合成的了，直接依据文本合成出原始频谱。其模型结构为encoder-decoder with attention 架构，是seq2seq常见模型。

Encoder目标时编码输入文本。结构：embedding + pre-net + CBHG

其中，pre-net为两层的NN，为瓶颈层，并使用dropout；CBHG结构为：convolution bank + highway networks + B-GRU

Decoder目标是使用注意力机制的RNN，再加输入和输出的一些变换。

### 评价

1. 模型复杂。用一个模型端到端方式合成语音，虽然省去了中间步骤，但模型复杂，不好调试，不好训练
2. 模型除错难。训练再好的模型，也可能对某些文本发音错误，这时，想要纠正这些错误，很难；需要重新准备数据、再次训练，再次训练也不一定能克服那些问题，代价很大。
3. 人为干预能力差。参数合成往往可以人为指定语速、重音、断句、停顿、韵律等信息，进行个性化合成。然而，端到端合成这些信息全部由模型自己学习，很难加入人为控制。因此，很难产品化。
4. 端到端不彻底。严格意义上讲，Tacotron也并非端到端，模型输出的是梅尔频谱(mel-scale spectrogram)，再用CBHG结构转为频谱幅度(spectral magnitude)，最后使用了Griffin-Lim这样的vocoder转为最终的音频。对此，好处是Tacotron中的seq2seq结构部分更容易训练，后处理部分可以单独训练，缺点是，后处理网络和Griffin-Lim本身的局限会影响到音质。

### Deep Voice 2

Tacotron发布后不久，百度的第二代Deep Voice [3]诞生了。Deep Voice 2和其前代相比最大变化是：1) 将Griffin-Lim替换成了WaveNet模型，2) 引入说话人向量，使Tacotron支持多说话人合成。

深度学习方法在语音合成上已经有大量应用（例如，[时长预测](https://arxiv.org/pdf/1606.06061)、[基频预测](https://arxiv.org/pdf/1608.06134)、[声学建模](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43266.pdf)、自回归语音生成 [4, 5]），但大多是单一说话人合成。对于多说话人合成，传统方法是先构建一个综合语音模型，然后再适配到不同的说话人。这一综合语音模型常用多说话人语料来训练，模型采用HMM或DNN，用i-vector、不同说话人输出层等方式来区别不同说话人，作为模型额外输入。

Deep voice 2可以合成多说话人，同时，不需要i-vector这种特征，而是模型自己学习说话人向量。前者好处不言而喻。后者好处是：1) 模型自己学到的说话人向量比i-vector这种特征更好；2) 不要说话人输出层，可以减少参数量，减少训练数据。

上面是宏观上看deep voice 2，下面我们具体看看其模型结构。从内容布局上，Deep Voice 2论文结构不是很紧密，做的工作比较多，这些工作或许足够拆开成3篇论文了。我们将按照以下三个部分分别介绍：1）对上代改进；2）Tacotron模型增加多说话人支持；3) Deep Voice模型增加多说话人支持。

#### 对Deep Voice 1的改进

1. 第一代使用一个模型同时联合预测音素时长和基频信息，第二代拆开成了两个模型：时长预测模型、基频预测模型，独立进行预测。
2. **音频切分**模型和上代结构不变，只是加入了batch normalization和残差连接。为了更好处理静音音素\<sil\>边界，对原始音频进行了平滑归一化处理，然后用一个固定阈值来确定是否为静音。
3. **时长预测**看成序列标注问题，将连续的时长离散化成一些bucket，模型为[CRF (Lample, 2016)](https://arxiv.org/pdf/1603.01360.pdf)。
4. **基频预测**使用时长和音素作为输入，输出*发音概率*和*基频取值*。模型结构使用B-GRU + FC结构。
5. **声学模型**和上代类似，使用改进后的WaveNet合成语音。稍作修改了：去掉1*1卷积、各层间使用共同bias

为什么要这样做？下面我们对上述每一点推测以下这样做的动机：

1. 联合预测还是独立预测时长和基频，取决于这些任务之间是否能相互促进、相互利用、存在共同点，可能是这两个任务之间关系不大，所以分开。
2. 批归一化和BN在别的领域效果不错，适当加入，也是可以的。
3. 离散化时长，或许是因为时长不需要非常精准，大概差不多就可以，对音质影响不大。这一点在上代论文中也提到了，音素边界存在**10-30毫秒**扰动对合成音质没什么影响。而且，离散化后，问题更好处理。
4. 基频预测只是预测是否发音和F0，更加合理，因为这两个任务更相关一些。
5. 上代花了那么多篇章讲WaveNet优化，本代稍作修改即可。

#### Tacotron增加多说人合成支持

这一部分是说如何改进Tacotron使其支持多说话人合成，这在Tacotron论文里其实也有提到。Deep Voice 2的做法是：千方百计加入说话人信息。说话人向量加入方式有四种模式：1) 说话人向量FC变换；2) RNN初始状态；3) 和输入做拼接；4) 激活函数作用前做点乘。

具体而言，在encoder、decoder、vocoder三处加入了说话人信息，注意，并没有在post processing net中加入。说话人信息，是自动从语音数据中学习一个16维的向量。让后将这个向量做一些变换，加入到原来的Tacotron模型中。

原始Tacotron使用Griffin-Lim算法将频谱转为幅度，百度使用WaveNet替换掉了Griffin-Lim。论文中给出的原因是：频谱中有少量噪音，将会导致Griffin-Lim算法产生的语音存在可以察觉的音质下降。因此，为了产生更好的音质，用WaveNet替代Griffin-Lim。

#### Deep Voice增加多说话人支持

需要改变Deep Voice 2模型中四大子结构，使其支持多说话人合成，其中G2P不用变，因为G2P和说话人没关系：

1. 音频切分模型，激活函数作用前，BN结果和说话人向量做点乘
2. 时长模型使用MLP-RNN-CRF，用说话人向量初始化RNN，并和输入做拼接
3. 基频预测，说话人向量作为RNN初始状态，也用到了其他地方
4. 声学模型，说话人向量和输入拼接，而非使用全局条件方式。

#### 评价

1. 从单一说话人合成看，联合训练改成独立训练，各模块任务更加明确，但同时模型数量变多了，增加复杂性。

2. 多说话人合成，相较于单一说话人合成，Deep Voice模型改动多，4/5都需变。

3. 文中提到用VCTK的108人训练多说话人合成，合成的这些人的声音和原始声音相似性如何，不得而知。用分类正确率虽然可以衡量合成的多说话人声音区分度，但也难以说明和原始声音的相似程度。

4. 用一个模型来合成多个说话人声音，而非每个说话人训一个模型，好处是找到不同人发音时的共同特点，共同的部分使用权值共享，不同的地方再单独定制；这样应该可以减少每个人语音数据量，取长补短，不失为未来合成的一个方向；然而究竟不同说话人间哪些参数应该共享，哪些参数因人而异，至关重要，本文留给了未来。

5. 在适配新说话人时，本文总结部分提到固定模型参数，只训练说话人向量。如果可以，或许可以合成任何人声音。然而，只训练说话人向量可能不够，还需训练和说话人向量相关的别的权值，本质还是分清可共享参数和不可共享参数。

6. 自学习说话人向量和i-vector作为多说话向量的区别，本文没有探讨。

### Deep Voice 3

一言以蔽之： Multi-speaker speech synthesis based on convolutional sequence-to-sequence model

卷积式序列到序列结构采用[Facebook 2017 ICML](https://arxiv.org/pdf/1705.03122.pdf)论文模型。

#### 动机揣测

*（注意：纯属个人瞎猜，不代表原作任何想法）*

WaveNet提出基于CNN的自回归生成式模型用于语音合成，将说话人信息、文本信息通过局部条件、全局条件的方式加入CNN各层中，每一预测下一采样点。

Tacotron使用端到端的方式进行语音合成，采用encoder-decoder的架构，用注意力机制建立起encoder和decoder的联系。decoder主要采用RNN结构来预测下一帧。

那么，能不能通过融合上述两篇文章的思想，提出一种新的模型结构。答案是肯定的，借鉴WaveNet的CNN结构，借鉴Tacotron每次预测一帧的思路，借鉴Tacotron encoder-decoder端到端的想法，于是诞生了Deep Voice 3。

既然都叫Deep Voice，那第三代和前两代有何区别和连系呢——没什么关系，除了和第二代中的多说话人思想延续了下了。Deep Voice 一代和二代主要思想是仿照参数合成步骤，每一阶段用一个NN替代。二代和一代相比，引入说话人向量，实现一个模型合成多个说话人声音。第三代则“颠覆"性的，走上了端到端的道路。

#### 模型特点

1. 全CNN，可全并行计算；
2. 因为1，所以训练速度**快**；
3. 采用单调注意力方式。

#### 评价

本文利用Facebook fairseq卷积encoder-decoder的框架，和Tacotron模型的思想，应用于端到端语音合成。然后比较了众多vocoder性能，发现还是从一代就开始用的WaveNet最好。

### Deep CNN TTS

和Deep Voice 3参考Facebook那篇卷积seq2seq一样，日本Tachibana也想到了这种移花接木之术。于是有了论文[9]。


## 参考文献

[1] Sercan Ömer Arik, Mike Chrzanowski, Adam Coates, Gregory Frederick Diamos, Andrew Gibiansky, Yongguo Kang, Xian Li, John Miller, Andrew Y. Ng, Jonathan Raiman, Shubho Sengupta, Mohammad Shoeybi: **Deep Voice: Real-time Neural Text-to-Speech**. ICML 2017: 195-204

[2] Wei Ping, Kainan Peng, Andrew Gibiansky, Sercan O.Arık, Ajay Kannan, Sharan Naran: **DEEP VOICE 3: 2000-SPEAKER NEURAL TEXT-TO-SPEECH**. CoRR abs/1710.07654 (2017)

[3] Sercan Ömer Arik, Gregory F. Diamos, Andrew Gibiansky, John Miller, Kainan Peng, Wei Ping, Jonathan Raiman, Yanqi Zhou: **Deep Voice 2: Multi-Speaker Neural Text-to-Speech**. CoRR abs/1705.08947 (2017)

[4] Aäron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, Koray Kavukcuoglu: **WaveNet: A Generative Model for Raw Audio**. CoRR abs/1609.03499 (2016)

[5] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron C. Courville, Yoshua Bengio: **SampleRNN: An Unconditional End-to-End Neural Audio Generation Model**. CoRR abs/1612.07837 (2016)

[6] Sotelo, J., Mehri, S., Kumar, K., Santos, J. F., Kastner, K., Courville, A., & Bengio, Y. (2017). **Char2Wav: End-to-end speech synthesis**.

[7] Yuxuan Wang, R. J. Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc V. Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous: **Tacotron: A Fully End-to-End Text-To-Speech Synthesis Model**. CoRR abs/1703.10135 (2017)

[8] Wang, W., Xu, S., & Xu, B. (2016). **First Step Towards End-to-End Parametric TTS Synthesis: Generating Spectral Parameters with Neural Attention**. *INTERSPEECH*.

[9] Tachibana, H., Uenoyama, K., & Aihara, S. (2017). **Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention**. *arXiv preprint arXiv:1710.08969*.

*仅为个人浅薄理解，或存在诸多纰漏，欢迎批评指正。*